#!/usr/bin/env python3
"""

"""
import os
from pathlib import Path
import tempfile

import datalad.api as dl
from datalad.utils import md5sum


def get_repo_path(
        output_base_dir: Path, study_id: str, visit_id: str) -> Path:
    return output_base_dir / study_id / f'{visit_id}_datalad'


def main(store_dir: str,
         study_id: str,
         visit_id: str):
    store_base_dir = Path(store_dir)
    # where to deposit the final datalad dataset
    repo_path = store_base_dir / study_id / f'{visit_id}_datalad'
    if repo_path.exists():
        # be safe
        raise ValueError(
            f'{repo_path} already exists, refusing to overwrite')
    # locate input tarball
    tar_path = store_base_dir / study_id / f'{visit_id}_dicom.tar'
    if not tar_path.exists():
        raise ValueError(f'no tarball at {tar_path}')

    with tempfile.TemporaryDirectory(prefix='dataladify_visit_') as wdir:
        runshit(
            # workdir
            wdir,
            # source visit tarball
            tar_path.resolve(),
            # source visit tarball URL
            f'https://some.base.url/{study_id}/{tar_path.name}',
            # path to deposit the repo at
            repo_path,
        )


def runshit(wdir, tarpath, tarurl, repopath):
    ds = dl.create(wdir)
    # alias for speed, `.repo` is really expensive
    repo = ds.repo
    # enable uncurl remote to have the tarball URL be claimed by it
    # and future-proof access (via its reconfiguration possibilities
    # without having to touch the annex record
    repo.call_annex([
        'initremote',
        'uncurl',
        'type=external',
        'externaltype=uncurl',
        'encryption=none',
    ])
    # we need its UUID later
    uncurl_uuid = repo.call_annex_records(['info', 'uncurl'])[0]['uuid']
    assert uncurl_uuid
    # register the URL of the tarball
    res = ds.addurls(
        [{
            'size': tarpath.stat().st_size,
            'md5': md5sum(tarpath),
            'path': str(Path('icf', tarpath.name)),
            'url': tarurl,
        }],
        '{url}',
        '{path}',
        key='et:MD5-s{size}--{md5}',
    )
    # fish out annex key of tarball.
    # we could also construct that, but let's not duplicate the setup above
    tarkey = [r.get('annexkey') for r in res
              if r.get('action') == 'fromkey'
              and r.get('path', '').endswith(tarpath.name)]
    assert len(tarkey) == 1
    tarkey = tarkey[0]
    assert tarkey
    # assure tar key availability
    repo.call_annex(['setpresentkey', tarkey, uncurl_uuid, '1'])

    # here we register the datalad-archives special remote, to claim
    # the dl+archives URLs registered below.
    # this really should be the archivist remote, but it is not yet implemented.
    # this will box ourselves into a corner, and force us to replace the
    # implementation of datalad-archives with the archivist code, rather than
    # have them coexist -- otherwise we would need to fix all datasets
    repo.call_annex([
        'initremote',
        'datalad-archives',
        'type=external',
        'externaltype=datalad-archives',
        'encryption=none',
    ])
    dlarchives_uuid = repo.call_annex_records(
        ['info', 'datalad-archives'])[0]['uuid']
    assert dlarchives_uuid
    dicoms = [
        dict(path=str(r['item']), md5=r['hash-md5'], size=r['size'])
        for r in dl.ls_file_collection(
            'tarfile',
            tarpath,
            hash=['md5'],
            result_renderer='disabled',
            return_type='generator')
        if r.get('type') == 'file'
    ]
    dicom_recs = ds.addurls(
        dicoms,
        f'dl+archive:{tarkey}#path={{path}}&size={{size}}',
        '{path}',
        key='et:MD5-s{size}--{md5}',
    )
    # assure availability for each DICOM
    for dicomkey in [
            r['annexkey']
            for r in dicom_recs if r.get('action') == 'fromkey'
    ]:
        repo.call_annex(['setpresentkey', dicomkey, dlarchives_uuid, '1'])

    repopath.mkdir()
    repo.call_git([
        'remote', 'add', 'icfstore',
        # we gain several levels of useless subdirectories :(
        f'datalad-annex::file://{repopath}?type=directory&directory={{path}}&encryption=none'
    ])
    ds.push(
        to='icfstore',
        # under no circumstances do we want to push annexed content.
        # and there also should be none
        data='nothing',
    )
    # some cleanup
    (repopath / 'tmp').rmdir()


if __name__ == '__main__':
    import argparse
    p = argparse.ArgumentParser(description=__doc__)
    p.add_argument(
        "-o", "--store-dir", metavar='PATH', default=os.getcwd(),
        help="Root directory of the ICF data store. "
        "Visit data will be read from it, and the DataLad dataset will be "
        "deposited into it."
    )
    p.add_argument(
        '--id', nargs=2, metavar=('STUDY-ID', 'VISIT-ID'), required=True,
        help="The study and visit identifiers, used to "
        "locate the visit archive in the storage organization. "
    )
    args = p.parse_args()
    main(store_dir=args.store_dir,
         study_id=args.id[0],
         visit_id=args.id[1],
    )
